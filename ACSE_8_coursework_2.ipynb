{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ACSE-8_coursework-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRscaXzlDzef"
      },
      "source": [
        "### Name: Ihebenachi Chigozirim\n",
        "### CID:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL3vQPYkhzOy"
      },
      "source": [
        "!pip install pycm livelossplot\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwm3nR2K5u98"
      },
      "source": [
        "#### Provided imports (add more you need them)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OeIX5M25t7K"
      },
      "source": [
        "import random\n",
        "import progressbar\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  \n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from livelossplot import PlotLosses\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBoy1vCb68TB"
      },
      "source": [
        "## 1-Prepare your LeNet-5 network [10 points]\n",
        "Use the code provided in the Jupyter Notebook template and modify it as you see fit to be able to perform a forward pass using the single dummy tensor input `x` provided. The lines of code that will do the forward pass and print the network are provided in the template."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RYabwg65rdL"
      },
      "source": [
        "#     make modifications in the code below\n",
        "\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet5, self).__init__()\n",
        "    self.c1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2) \n",
        "    self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)   \n",
        "    self.c3 = nn.Conv2d(6, 16, kernel_size=5, stride=1)   \n",
        "    self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.c5 = nn.Linear(16*5*5, 120)    \n",
        "    self.f6 = nn.Linear(120, 84)   \n",
        "    self.output = nn.Linear(84, 10)  \n",
        "    self.act = nn.ReLU()           \n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.act(self.c1(x))    \n",
        "    x = self.act(self.s2(x))   \n",
        "    x = self.act(self.c3(x))   \n",
        "    x = self.act(self.s4(x))    \n",
        "    x = x.view(-1, x.size(1)*x.size(2)*x.size(3))     \n",
        "    x = self.act(self.c5(x))       \n",
        "    x = self.act(self.f6(x))    \n",
        "    return self.output(x)          \n",
        "  \n",
        "# dummy input of the same size as the CIFAR-10 images\n",
        "x = torch.ones((1, 3, 32, 32))\n",
        "model = LeNet5()\n",
        "y = model(x)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YMisK8y8CDU"
      },
      "source": [
        "## 2-Load CIFAR-10 [10 points]\n",
        "Use `torchvision.datasets.CIFAR10` to load the CIFAR-10 dataset (training and test sets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfJY68Sx8-nn"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl5euEpd_sUj"
      },
      "source": [
        "## 3-Plot data [5 points]\n",
        "Plot 25 images of the training set together with their corresponding label names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzjg5It0_r8e"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDo89tMa9F75"
      },
      "source": [
        "## 4-Create a training, validation split [5 points]\n",
        "Split the data using `sklearn.model\\_selection.StratifiedShuffleSplit`:\n",
        "\n",
        "- 90\\% of the data in the training set\n",
        "- 10\\% of the data in the validation set\n",
        "\n",
        "Prepare the downloaded datasets to be used with your modified network in **1-Prepare your LeNet-5 network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hqepj8NYw57"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsv6RASWAUCC"
      },
      "source": [
        "## 5-Grid search [20 points]\n",
        "From  the  list  below,  select  two  hyperparameters  and  perform  a  2D  grid-search  to  find  the  optimal values for these two hyperparameters.  The range of values to test are provided.  Justify your choice of the two hyperparameters you want to tune (write a paragraph in a markdown cell explaining why you chose these two particular parameters). \n",
        "\n",
        "The list of hyperparameters to choose from is:\n",
        "\n",
        "a)  Random Number Seed:  **42**  [31, 42, 53] \\\n",
        "b)  Learning Rate:  **1e-2**  [1e-1, 1e-2, 1e-3] \\\n",
        "c)  Momentum:  **0.5**  [0.2, 0.5, 0.8] \\\n",
        "d)  Batch Size:  **64**  [64, 128, 640] \\\n",
        "e)  Number of epochs:  **30**  [10, 30, 50]\n",
        "\n",
        "The **values in bold** next to each hyperparameter are the values you need to use if you are not tuning this particular hyperparameter.  The values between square brackets are the values to use if you choose to tune this particular hyperparameter. Fixed hyperparameters:\n",
        "\n",
        "- Optimiser:SGD+momentum\n",
        "- Test batch size:1000\n",
        "\n",
        "Write the results in two tables (one for the loss and one for the accuracy) where the columns and rows are the first and second hyperparameter have selected. You can use markdown tables or create the table in python.\n",
        "\n",
        "You don’t need to plot all the livelossplotplots for each combination you try, as the results will be summarised in the table, but at least plot two of the grid-search runs. \n",
        "\n",
        "Select the best values for the two hyperparameters you have chosen to optimise and **justify your choice**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl2FDyyyoGon"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qvD9_72B3oV"
      },
      "source": [
        "## 6-Train with best hyperparameters [5 points]\n",
        "\n",
        "Once you have your two best hyperparameters, retrain the model by combining the validation, training, and test sets as you see fit. Report the final accuracy on the test set. Use `livelossplot` to plot the values of the training evolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CzNWdXTErVp"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ka9eNo7EuVF"
      },
      "source": [
        "## 7-Answer the following questions [1 point each]\n",
        "Which of these data-augmentation transforms would be reasonable to apply to CIFAR10 and why? \n",
        "\n",
        "1. Left-Right Flips\n",
        "2. Random Rotations by up to 10 Degrees\n",
        "3. Up-Down Flips\n",
        "4. Shifting up-down, left-right by 5 pixels\n",
        "5. Contrast Changes\n",
        "6. Adding Gaussian Noise\n",
        "7. Random Rotations by up to 90 Degrees\n",
        "\n",
        "Justify each one of your answers.\n",
        "\n",
        "---\n",
        "\n",
        "Write your answers here:\n",
        "\n",
        "1. \n",
        "2. \n",
        "3. \n",
        "4. \n",
        "5. \n",
        "6. \n",
        "7. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZcLQ44cFIBi"
      },
      "source": [
        "## 8-Plot augmented data [13 points]\n",
        "\n",
        "Select one of the data-augmentation transforms you decided were reasonable in the previous question. Implement it, and apply it to 9 images of the CIFAR-10 dataset.\n",
        "\n",
        "Plot the 9 transformed images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lAyyugRFqwp"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDk6JOtyGKZe"
      },
      "source": [
        "## 9-Visualising loss landscapes paper [10 points]\n",
        "\n",
        "Read the provided paper [Visualising the Loss Landscape of Neural Nets](https://arxiv.org/pdf/1712.09913.pdf).\n",
        "\n",
        "This paper is contains a lot of advanced concepts. You only need to read and understand it well up to, and including, section 4, (Proposed Visualization: Filter-Wise Normalisation) to answer the questions below. In section 4 you don't need to fully understand the rationale for doing Filter-Wise Normalisation, but you do need to understand what Filter-Wise Normalisation is.\n",
        "\n",
        "Answer the following questions (in a markdown cell):\n",
        "\n",
        "1. What are $\\delta$, $\\eta$, $\\alpha$ and $\\beta$ in equation (1)? [5 points]\n",
        "2. What does Filter-Wise Normalisation do? [5 points] Don't need to explain the reasons for doing it, just how it modifies the random directions $\\delta$ and $\\eta$. \n",
        "\n",
        "Explain well and justify your answers. **(Don't answer in 1. that $\\delta$ is a random direction just because it says that in 2. Explain what is meant by a random direction well.)**\n",
        "\n",
        "---\n",
        "\n",
        "Write you answers here:\n",
        "\n",
        "1. \n",
        "\n",
        "\n",
        "2. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-iB3cUwH86e"
      },
      "source": [
        "## 10-Visualise loss landscape [15 points]\n",
        "\n",
        "Use the formula described in equation (1) in the paper in combination with the Filter-Wise Normalisation to generate landscape plots. For that use your final trained model (output of question **7**) and 25 values for $\\alpha$ and 25 values for $\\beta$ to generate a 2D plot with a 100 points. Use the provided snippets of code in the Jupyter Notebook template to assist you in generating the plots and to guide you in the functions you will need to implement (not mandatory, you can implement everything from scratch if you prefer).\n",
        "\n",
        "Note that in this question you will not be comparing the smoothness of different loss landscapes (as they do in the paper), you will only be plotting the loss function landscape around the loss value corresponding to your trained network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxoB2bavIeUe"
      },
      "source": [
        "# The following snippets of code are only to assist you. You can decide to use \n",
        "# them or not. They are only intended to provide you with some functionality\n",
        "#  you may find useful when trying to generate the loss landscape plots.\n",
        "\n",
        "\n",
        "# function to create random directions:\n",
        "def create_random_directions(weights, ignore1D=False, seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    direction = [torch.randn(w.size()).to(device) for w in weights]\n",
        "    \n",
        "    # apply filter normalisation, where every perturbation d in direction has the same norm as its corresponding w in weights\n",
        "    for d, w in zip(direction, weights):\n",
        "        if ignore1D and d.dim() <= 1:\n",
        "            d.fill_(0)\n",
        "        d.mul_(w.norm()/(d.norm() + 1e-10)) # add small perturbation to avoid division by zero\n",
        "\n",
        "    return direction\n",
        "\n",
        "\n",
        "# function to update weigths\n",
        "def update_weights(model, origin_weights, x_dir, y_dir, dx=0.1, dy=0.1):\n",
        "    updates = [x.to(device)*dx + y.to(device)*dy for (x, y) in zip(x_dir, y_dir)]\n",
        "    for (p, w, u) in zip(model.parameters(), origin_weights, updates):\n",
        "        p.data = w + u\n",
        "    return None\n",
        "\n",
        "\n",
        "# function to plot loss landscape as a surface\n",
        "def plot_loss_landscape(xx, yy, loss_landscape):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8),subplot_kw={\"projection\": \"3d\"})\n",
        "    surf = ax.plot_surface(xx, yy, loss_landscape, cmap='viridis', edgecolor='none',\n",
        "                       linewidth=0, antialiased=True,  rstride=1, cstride=1,)\n",
        "    ax.set_xlabel(r'X')\n",
        "    ax.set_ylabel(r'Y')\n",
        "    ax.set_zlabel(r'Loss')\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# function to plot loss landscape as a contour\n",
        "def contour_loss_landscape(xx, yy, loss_landscape):\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "    surf = ax.contourf(xx, yy, loss_landscape, cmap='viridis', levels=100)\n",
        "    ax.set_xlabel(r'X')\n",
        "    ax.set_ylabel(r'Y')\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# BONUS: functions to compute the angle between 2 random vectors\n",
        "# in high-dimensional spaces two random vectors are quite likely to be\n",
        "# orthogonal (or almost). No points involved here, this is just for fun!\n",
        "#\n",
        "def angle(vec1, vec2):\n",
        "    return torch.acos(torch.dot(vec1, vec2)/(vec1.norm()*vec2.norm())).item()\n",
        "\n",
        "def rad2deg(angle):\n",
        "    return angle*180/np.pi\n",
        "\n",
        "def concat_torch_list(torch_list):\n",
        "    for i, t in enumerate(torch_list):\n",
        "        torch_list[i] = t.flatten()\n",
        "    return torch.cat(torch_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxVz-FhvKPUI"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cj5hz6lI4Kl"
      },
      "source": [
        "### 10-(continued)\n",
        "\n",
        "Discuss your results, and justify the choices you make along the generation process of plotting the loss landscapes (for example the range your choose for your $\\alpha$ and $\\beta$ values).\n",
        "\n",
        "---\n",
        "\n",
        "Write your answers here:"
      ]
    }
  ]
}